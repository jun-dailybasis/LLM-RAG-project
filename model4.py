import os
import pickle
import numpy as np
import pandas as pd
from langchain_openai import ChatOpenAI
from langchain.docstore.document import Document
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.schema import SystemMessage, HumanMessage
from sklearn.metrics.pairwise import cosine_similarity

# ------------------------------------------------------------------
# [1] ì„ë² ë”© ìºì‹œ ë¡œë“œ/ì €ì¥
# ------------------------------------------------------------------
EMBEDDING_CACHE_FILE = "./embeddings_cache3.pkl"

def load_embeddings_cache():
    """ ë¡œì»¬ ìºì‹œ íŒŒì¼ì—ì„œ ì„ë² ë”© ì •ë³´ë¥¼ ë¡œë“œ """
    if os.path.exists(EMBEDDING_CACHE_FILE):
        with open(EMBEDDING_CACHE_FILE, "rb") as f:
            return pickle.load(f)
    return {}

def save_embeddings_cache(cache):
    """ ë¡œì»¬ ìºì‹œì— ì„ë² ë”© ì •ë³´ë¥¼ ì €ì¥ """
    with open(EMBEDDING_CACHE_FILE, "wb") as f:
        pickle.dump(cache, f)

# ------------------------------------------------------------------
# [2] ì—‘ì…€ ë¡œë“œ â†’ (body ìƒì„±) â†’ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
# ------------------------------------------------------------------
def load_and_prepare_data(file_path):
    df = pd.read_excel(file_path, engine="openpyxl")

    # 1) Body ì •ì˜
    df['body'] = df.apply(
        lambda row: (
            f"ì œí’ˆëª…: {row['ì œí’ˆëª…'] if pd.notnull(row['ì œí’ˆëª…']) else 'ì •ë³´ ì—†ìŒ'}\n"
            f"ì œì¡°ì—…ì²´: {row['ì—…ì²´ëª…'] if pd.notnull(row['ì—…ì²´ëª…']) else 'ì •ë³´ ì—†ìŒ'}\n"
            f"ì£¼ì„±ë¶„: {row['ì£¼ì„±ë¶„'] if pd.notnull(row['ì£¼ì„±ë¶„']) else 'ì •ë³´ ì—†ìŒ'}\n"
            f"íš¨ëŠ¥: {row['ì´ ì•½ì˜ íš¨ëŠ¥ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?'] if pd.notnull(row['ì´ ì•½ì˜ íš¨ëŠ¥ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?']) else 'ì •ë³´ ì—†ìŒ'}\n"
            f"ë³µìš©ë²•: {row['ì´ ì•½ì€ ì–´ë–»ê²Œ ì‚¬ìš©í•©ë‹ˆê¹Œ?'] if pd.notnull(row['ì´ ì•½ì€ ì–´ë–»ê²Œ ì‚¬ìš©í•©ë‹ˆê¹Œ?']) else 'ì •ë³´ ì—†ìŒ'}\n"
            f"ì‚¬ìš© ì „ ì£¼ì˜ì‚¬í•­: {row['ì´ ì•½ì„ ì‚¬ìš©í•˜ê¸° ì „ì— ë°˜ë“œì‹œ ì•Œì•„ì•¼ í•  ë‚´ìš©ì€ ë¬´ì—‡ì…ë‹ˆê°€?'] if pd.notnull(row['ì´ ì•½ì„ ì‚¬ìš©í•˜ê¸° ì „ì— ë°˜ë“œì‹œ ì•Œì•„ì•¼ í•  ë‚´ìš©ì€ ë¬´ì—‡ì…ë‹ˆê°€?']) else 'ì •ë³´ ì—†ìŒ'}\n"
            f"ì‚¬ìš©ìƒ ì£¼ì˜ì‚¬í•­: {row['ì´ ì•½ì˜ ì‚¬ìš©ìƒ ì£¼ì˜ì‚¬í•­ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?'] if pd.notnull(row['ì´ ì•½ì˜ ì‚¬ìš©ìƒ ì£¼ì˜ì‚¬í•­ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?']) else 'ì •ë³´ ì—†ìŒ'}\n"
            f"ì£¼ì˜í•´ì•¼ í•  ì•½/ìŒì‹: {row['ì´ ì•½ì„ ì‚¬ìš©í•˜ëŠ” ë™ì•ˆ ì£¼ì˜í•´ì•¼ í•  ì•½ ë˜ëŠ” ìŒì‹ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?'] if pd.notnull(row['ì´ ì•½ì„ ì‚¬ìš©í•˜ëŠ” ë™ì•ˆ ì£¼ì˜í•´ì•¼ í•  ì•½ ë˜ëŠ” ìŒì‹ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?']) else 'ì •ë³´ ì—†ìŒ'}\n"
            f"ë¶€ì‘ìš©: {row['ì´ ì•½ì€ ì–´ë–¤ ì´ìƒë°˜ì‘ì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆê¹Œ?'] if pd.notnull(row['ì´ ì•½ì€ ì–´ë–¤ ì´ìƒë°˜ì‘ì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆê¹Œ?']) else 'ì •ë³´ ì—†ìŒ'}\n"
            f"ë³´ê´€ë°©ë²•: {row['ì´ ì•½ì€ ì–´ë–»ê²Œ ë³´ê´€í•´ì•¼ í•©ë‹ˆê¹Œ?'] if pd.notnull(row['ì´ ì•½ì€ ì–´ë–»ê²Œ ë³´ê´€í•´ì•¼ í•©ë‹ˆê¹Œ?']) else 'ì •ë³´ ì—†ìŒ'}\n"
        ),
        axis=1
    )

    # 2) Document ê°ì²´ë¡œ ë³€í™˜
    documents = [
        Document(
            page_content=row['body'],
            metadata={
                "product_name": row['ì œí’ˆëª…'] if pd.notnull(row['ì œí’ˆëª…']) else 'ì •ë³´ ì—†ìŒ',
                "company_name": row['ì—…ì²´ëª…'] if pd.notnull(row['ì—…ì²´ëª…']) else 'ì •ë³´ ì—†ìŒ',
                "main_ingredient": row['ì£¼ì„±ë¶„'] if pd.notnull(row['ì£¼ì„±ë¶„']) else 'ì •ë³´ ì—†ìŒ'
            }
        )
        for _, row in df.iterrows()
    ]
    return documents

# ------------------------------------------------------------------
# [3] ì„ë² ë”© & ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
# ------------------------------------------------------------------
def build_vector_store(documents):
    """
    ì „ì²´ documents ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•´ ì„ë² ë”©ì„ ìƒì„±/ë¡œë“œí•˜ê³ 
    numpy array(vectors)ì™€ ë¬¸ì„œ ëª©ë¡(filtered_documents)ì„ ë°˜í™˜.
    """
    embedding_model = OpenAIEmbeddings(model='text-embedding-3-large')
    embedding_cache = load_embeddings_cache()
    new_embeddings = {}

    vectors = []
    filtered_documents = []

    for doc in documents:
        content = doc.page_content
        if content in embedding_cache:
            print(f"âœ… Using cached embedding for: {content[:30]}...")
            vectors.append(embedding_cache[content])
        else:
            print(f"ğŸš¨ Generating new embedding for: {content[:30]}...")
            embedding = embedding_model.embed_query(content)
            new_embeddings[content] = embedding
            vectors.append(embedding)

        filtered_documents.append(doc)

    # ìƒˆë¡œ ìƒì„±ëœ ì„ë² ë”©ì„ ìºì‹œì— ì—…ë°ì´íŠ¸í•˜ê³  ì €ì¥
    embedding_cache.update(new_embeddings)
    save_embeddings_cache(embedding_cache)

    return np.array(vectors), filtered_documents

# ------------------------------------------------------------------
# [4] ìœ ì‚¬ë„ ê²€ìƒ‰ í•¨ìˆ˜
# ------------------------------------------------------------------
def similarity_search(question, vectors, documents, embedding_model, top_k=5):
    """
    ì§ˆë¬¸ì„ ì„ë² ë”©í•´, ì „ì²´ ë²¡í„° ì¤‘ ìƒìœ„ top_kê°œë¥¼ ê³¨ë¼ ê·¸ ë¬¸ì„œë¥¼ ë°˜í™˜
    """
    question_embedding = embedding_model.embed_query(question)
    similarities = cosine_similarity([question_embedding], vectors)[0]
    top_indices = np.argsort(similarities)[::-1][:top_k]
    return [documents[i] for i in top_indices]

# ------------------------------------------------------------------
# [5] RAG ê¸°ë°˜ ì§ˆì˜ ì‘ë‹µ
# ------------------------------------------------------------------
def query_rag(question, vectors, documents, model, embedding_model, k=5):
    # 1) ìœ ì‚¬ë„ ê²€ìƒ‰
    search_results = similarity_search(question, vectors, documents, embedding_model, top_k=k)

    if not search_results:
        return "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # 2) ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš© í•©ì¹˜ê¸°
    context = "\n\n".join([doc.page_content for doc in search_results])

    # 3) Few-shot ì˜ˆì‹œ ì •ì˜ (ë¬¸ìì—´ë¡œ ë°”ë¡œ ì •ì˜)
    few_shot = """
ì˜ˆì‹œ 1:
ì§ˆë¬¸: íƒ€ì´ë ˆë†€ì˜ íš¨ëŠ¥ì€ ë¬´ì—‡ì¸ê°€ìš”?
ë‹µë³€: íƒ€ì´ë ˆë†€ì€ ë‘í†µ ë° ë°œì—´ ì™„í™”ì— íš¨ê³¼ì ì…ë‹ˆë‹¤.

ì˜ˆì‹œ 2:
ì§ˆë¬¸: ì´ë¶€í”„ë¡œíœì˜ ë³µìš©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?
ë‹µë³€: ì´ë¶€í”„ë¡œíœì€ ì„±ì¸ì˜ ê²½ìš° 1ì¼ 3íšŒ, ì‹ì‚¬ í›„ ë³µìš©í•˜ëŠ” ê²ƒì´ ê¶Œì¥ë©ë‹ˆë‹¤.

ì˜ˆì‹œ 3:
ì§ˆë¬¸: íŒí”¼ë¦°ì˜ ë¶€ì‘ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?
ë‹µë³€: íŒí”¼ë¦°ì€ ì¡¸ìŒ, êµ¬ê°• ê±´ì¡°, ì†Œí™”ë¶ˆëŸ‰ ë“±ì˜ ë¶€ì‘ìš©ì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì˜ˆì‹œ 4:
ì§ˆë¬¸: ì•„ìŠ¤í”¼ë¦°ê³¼ íƒ€ì´ë ˆë†€ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?
ë‹µë³€: ì•„ìŠ¤í”¼ë¦°ì€ í•­ì—¼ì¦ íš¨ê³¼ê°€ ìˆìœ¼ë©°, íƒ€ì´ë ˆë†€ì€ ì£¼ë¡œ ì§„í†µê³¼ í•´ì—´ì— ì‚¬ìš©ë©ë‹ˆë‹¤.

ì˜ˆì‹œ 5:
ì§ˆë¬¸: ë¶€ë£¨íœì˜ ì‚¬ìš©ìƒ ì£¼ì˜ì‚¬í•­ì€ ë¬´ì—‡ì¸ê°€ìš”?
ë‹µë³€: ë¶€ë£¨íœì€ ìœ„ì¥ ì¥ì• ë¥¼ ì¼ìœ¼í‚¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì‹ì‚¬ í›„ ë³µìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ë˜í•œ, ì¥ê¸° ë³µìš©ì€ í”¼í•˜ì„¸ìš”.
"""

    # 4) ê°œì„ ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    system_prompt = f"""
ë„ˆëŠ” ì „ë¬¸ ì•½ì‚¬(Pharmacist) ì—­í• ì„ ë§¡ê³  ìˆë‹¤.
ì‚¬ìš©ìì˜ ì˜ì•½í’ˆ ê´€ë ¨ ì§ˆë¬¸ì— ëŒ€í•´, ì•„ë˜ ì œê³µëœ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬
ì „ë¬¸ì ì´ë©´ì„œë„ ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹µë³€í•´ë¼.

ë‹¤ìŒì€ ì§ˆë¬¸ê³¼ ë‹µë³€ ì˜ˆì‹œì´ë‹¤:
{few_shot}

- ì œê³µëœ ì •ë³´ì— ì—†ëŠ” ë‚´ìš©ì´ë‚˜ í™•ì‹ ì´ ì—†ëŠ” ë¶€ë¶„ì— ëŒ€í•´ì„œëŠ” 'ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤' ë˜ëŠ” 'í•´ë‹¹ ì •ë³´ë¡œëŠ” ë‹µí•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤'ë¼ê³  ëª…í™•í•˜ê²Œ ë‹µí•˜ë¼.
- ë°˜ë“œì‹œ ì œê³µëœ ì •ë³´ë§Œì„ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ë¼.
- ë‹µë³€ì€ ê°„ê²°í•˜ë©´ì„œë„ êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•˜ë¼.

ì°¸ê³ í•  ìˆ˜ ìˆëŠ” ì •ë³´:
{context}
"""

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=question),
    ]

    # 5) ëª¨ë¸ ì§ˆì˜
    response = model.invoke(messages)
    return response.content

# ------------------------------------------------------------------
# [6] ë©”ì¸ ì‹¤í–‰ë¶€
# ------------------------------------------------------------------
if __name__ == "__main__":
    file_path = "./edruginfo.xlsx"

    # ë°ì´í„° ë¡œë“œ
    documents = load_and_prepare_data(file_path)

    # ì„ë² ë”© + ìºì‹±
    vectors, filtered_documents = build_vector_store(documents)

    # LLM & Embedding ëª¨ë¸ ì¤€ë¹„
    llm_model = ChatOpenAI(model='gpt-4o-mini', temperature=0)
    embedding_model = OpenAIEmbeddings(model='text-embedding-3-large')

    while True:
        question = input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥): ")
        if question.lower() == 'exit':
            break

        answer = query_rag(question, vectors, filtered_documents, llm_model, embedding_model, k=5)
        print("\nA:", answer)
