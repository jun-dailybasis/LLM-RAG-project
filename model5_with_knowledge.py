import os
import pickle
import numpy as np
import pandas as pd
from langchain_openai import ChatOpenAI
from langchain.docstore.document import Document
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.schema import SystemMessage, HumanMessage
from sklearn.metrics.pairwise import cosine_similarity

# ------------------------------------------------------------------
# [1] ì„ë² ë”© ìºì‹œ ë¡œë“œ/ì €ì¥
# ------------------------------------------------------------------
EMBEDDING_CACHE_FILE = "./embeddings_cache3.pkl"

def load_embeddings_cache():
    """ ë¡œì»¬ ìºì‹œ íŒŒì¼ì—ì„œ ì„ë² ë”© ì •ë³´ë¥¼ ë¡œë“œ """
    if os.path.exists(EMBEDDING_CACHE_FILE):
        with open(EMBEDDING_CACHE_FILE, "rb") as f:
            return pickle.load(f)
    return {}

def save_embeddings_cache(cache):
    """ ë¡œì»¬ ìºì‹œì— ì„ë² ë”© ì •ë³´ë¥¼ ì €ì¥ """
    with open(EMBEDDING_CACHE_FILE, "wb") as f:
        pickle.dump(cache, f)

# ------------------------------------------------------------------
# [2] ì—‘ì…€ ë¡œë“œ â†’ (body ìƒì„±) â†’ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
# ------------------------------------------------------------------
def load_and_prepare_data(file_path):
    df = pd.read_excel(file_path, engine="openpyxl")

    # 1) Body ì •ì˜
    df['body'] = df.apply(
        lambda row: (
            f"ì œí’ˆëª…: {row['ì œí’ˆëª…'] if pd.notnull(row['ì œí’ˆëª…']) else 'ì •ë³´ ì—†ìŒ'}\n"
            f"ì œì¡°ì—…ì²´: {row['ì—…ì²´ëª…'] if pd.notnull(row['ì—…ì²´ëª…']) else 'ì •ë³´ ì—†ìŒ'}\n"
            f"ì£¼ì„±ë¶„: {row['ì£¼ì„±ë¶„'] if pd.notnull(row['ì£¼ì„±ë¶„']) else 'ì •ë³´ ì—†ìŒ'}\n"
            f"íš¨ëŠ¥: {row['ì´ ì•½ì˜ íš¨ëŠ¥ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?'] if pd.notnull(row['ì´ ì•½ì˜ íš¨ëŠ¥ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?']) else 'ì •ë³´ ì—†ìŒ'}\n"
            f"ë³µìš©ë²•: {row['ì´ ì•½ì€ ì–´ë–»ê²Œ ì‚¬ìš©í•©ë‹ˆê¹Œ?'] if pd.notnull(row['ì´ ì•½ì€ ì–´ë–»ê²Œ ì‚¬ìš©í•©ë‹ˆê¹Œ?']) else 'ì •ë³´ ì—†ìŒ'}\n"
            f"ì‚¬ìš© ì „ ì£¼ì˜ì‚¬í•­: {row['ì´ ì•½ì„ ì‚¬ìš©í•˜ê¸° ì „ì— ë°˜ë“œì‹œ ì•Œì•„ì•¼ í•  ë‚´ìš©ì€ ë¬´ì—‡ì…ë‹ˆê°€?'] if pd.notnull(row['ì´ ì•½ì„ ì‚¬ìš©í•˜ê¸° ì „ì— ë°˜ë“œì‹œ ì•Œì•„ì•¼ í•  ë‚´ìš©ì€ ë¬´ì—‡ì…ë‹ˆê°€?']) else 'ì •ë³´ ì—†ìŒ'}\n"
            f"ì‚¬ìš©ìƒ ì£¼ì˜ì‚¬í•­: {row['ì´ ì•½ì˜ ì‚¬ìš©ìƒ ì£¼ì˜ì‚¬í•­ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?'] if pd.notnull(row['ì´ ì•½ì˜ ì‚¬ìš©ìƒ ì£¼ì˜ì‚¬í•­ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?']) else 'ì •ë³´ ì—†ìŒ'}\n"
            f"ì£¼ì˜í•´ì•¼ í•  ì•½/ìŒì‹: {row['ì´ ì•½ì„ ì‚¬ìš©í•˜ëŠ” ë™ì•ˆ ì£¼ì˜í•´ì•¼ í•  ì•½ ë˜ëŠ” ìŒì‹ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?'] if pd.notnull(row['ì´ ì•½ì„ ì‚¬ìš©í•˜ëŠ” ë™ì•ˆ ì£¼ì˜í•´ì•¼ í•  ì•½ ë˜ëŠ” ìŒì‹ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?']) else 'ì •ë³´ ì—†ìŒ'}\n"
            f"ë¶€ì‘ìš©: {row['ì´ ì•½ì€ ì–´ë–¤ ì´ìƒë°˜ì‘ì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆê¹Œ?'] if pd.notnull(row['ì´ ì•½ì€ ì–´ë–¤ ì´ìƒë°˜ì‘ì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆê¹Œ?']) else 'ì •ë³´ ì—†ìŒ'}\n"
            f"ë³´ê´€ë°©ë²•: {row['ì´ ì•½ì€ ì–´ë–»ê²Œ ë³´ê´€í•´ì•¼ í•©ë‹ˆê¹Œ?'] if pd.notnull(row['ì´ ì•½ì€ ì–´ë–»ê²Œ ë³´ê´€í•´ì•¼ í•©ë‹ˆê¹Œ?']) else 'ì •ë³´ ì—†ìŒ'}\n"
        ),
        axis=1
    )

    # 2) Document ê°ì²´ë¡œ ë³€í™˜
    documents = [
        Document(
            page_content=row['body'],
            metadata={
                "product_name": row['ì œí’ˆëª…'] if pd.notnull(row['ì œí’ˆëª…']) else 'ì •ë³´ ì—†ìŒ',
                "company_name": row['ì—…ì²´ëª…'] if pd.notnull(row['ì—…ì²´ëª…']) else 'ì •ë³´ ì—†ìŒ',
                "main_ingredient": row['ì£¼ì„±ë¶„'] if pd.notnull(row['ì£¼ì„±ë¶„']) else 'ì •ë³´ ì—†ìŒ'
            }
        )
        for _, row in df.iterrows()
    ]
    return documents

# ------------------------------------------------------------------
# [3] ì„ë² ë”© & ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
# ------------------------------------------------------------------
def build_vector_store(documents):
    """
    ì „ì²´ documents ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•´ ì„ë² ë”©ì„ ìƒì„±/ë¡œë“œí•˜ê³ 
    numpy array(vectors)ì™€ ë¬¸ì„œ ëª©ë¡(filtered_documents)ì„ ë°˜í™˜.
    """
    embedding_model = OpenAIEmbeddings(model='text-embedding-3-large')
    embedding_cache = load_embeddings_cache()
    new_embeddings = {}

    vectors = []
    filtered_documents = []

    for doc in documents:
        content = doc.page_content
        if content in embedding_cache:
            print(f"âœ… Using cached embedding for: {content[:30]}...")
            vectors.append(embedding_cache[content])
        else:
            print(f"ğŸš¨ Generating new embedding for: {content[:30]}...")
            embedding = embedding_model.embed_query(content)
            new_embeddings[content] = embedding
            vectors.append(embedding)

        filtered_documents.append(doc)

    # ìƒˆë¡œ ìƒì„±ëœ ì„ë² ë”©ì„ ìºì‹œì— ì—…ë°ì´íŠ¸í•˜ê³  ì €ì¥
    embedding_cache.update(new_embeddings)
    save_embeddings_cache(embedding_cache)

    return np.array(vectors), filtered_documents

# ------------------------------------------------------------------
# [4] ìœ ì‚¬ë„ ê²€ìƒ‰ í•¨ìˆ˜
# ------------------------------------------------------------------
def similarity_search(question, vectors, documents, embedding_model, top_k=5):
    """
    ì§ˆë¬¸ì„ ì„ë² ë”©í•´, ì „ì²´ ë²¡í„° ì¤‘ ìƒìœ„ top_kê°œë¥¼ ê³¨ë¼ ê·¸ ë¬¸ì„œë¥¼ ë°˜í™˜
    """
    question_embedding = embedding_model.embed_query(question)
    similarities = cosine_similarity([question_embedding], vectors)[0]
    top_indices = np.argsort(similarities)[::-1][:top_k]
    return [documents[i] for i in top_indices]

# ------------------------------------------------------------------
# [5] RAG ê¸°ë°˜ ì§ˆì˜ ì‘ë‹µ
# ------------------------------------------------------------------
def query_rag(question, vectors, documents, model, embedding_model, k=5):
    # 1) ìœ ì‚¬ë„ ê²€ìƒ‰
    search_results = similarity_search(question, vectors, documents, embedding_model, top_k=k)

    if not search_results:
        # ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ
        fallback_prompt = f"""
ë„ˆëŠ” ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ì•½ì‚¬(Pharmacist) ì—­í• ì„ ë§¡ê³  ìˆë‹¤.
ì‚¬ìš©ìê°€ ì˜ì•½í’ˆ ê´€ë ¨ ì§ˆë¬¸ì„ í–ˆìœ¼ë‚˜, ì°¸ê³ í•  ì •ë³´ê°€ ì¶©ë¶„í•˜ì§€ ì•Šë‹¤.

- ì´ ê²½ìš°, ê°€ëŠ¥í•œ ì¼ë°˜ì ì¸ ì˜ì•½í’ˆ ê´€ë ¨ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ë¼.
- ë§Œì•½ ëª…í™•í•œ ë‹µë³€ì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤ë©´, 'ì´ ì§ˆë¬¸ì— ëŒ€í•œ ì •í™•í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ...'ë¼ëŠ” í˜•ì‹ìœ¼ë¡œ ë‹µí•˜ë¼.
- ë‹µë³€ì€ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ê³  ë”°ëœ»í•œ ì–´ì¡°ë¡œ ì‘ì„±í•˜ë¼.

ì‚¬ìš©ì ì§ˆë¬¸:
{question}
"""
        messages = [
            SystemMessage(content=fallback_prompt),
            HumanMessage(content=question),
        ]
        response = model.invoke(messages)
        return response.content

    # 2) ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš© í•©ì¹˜ê¸°
    context = "\n\n".join([doc.page_content for doc in search_results])

    # 3) ì¹œì ˆí•˜ê³  ìì„¸í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    system_prompt = f"""
ë„ˆëŠ” ì‚¬ìš©ìê°€ ì•½ì‚¬ì—ê²Œ ë¬»ëŠ” ê²ƒì²˜ëŸ¼ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” ì•½ì‚¬ì´ë‹¤.
ë‹¤ìŒì€ ë‹µë³€ ì‘ì„± ì‹œì˜ ì£¼ì˜ì‚¬í•­ì´ë‹¤:
- ì‚¬ìš©ìì˜ ê±±ì •ê³¼ ê¶ê¸ˆì¦ì„ ê³µê°í•˜ë©° ë‹µí•˜ë¼.
- ì œê³µëœ ì •ë³´ì—ì„œë§Œ ê·¼ê±°ë¥¼ ì°¾ë˜, ì´í•´í•˜ê¸° ì‰½ê³  ì¹œì ˆí•œ ì–´ì¡°ë¥¼ ìœ ì§€í•˜ë¼.
- ì •ë³´ê°€ ë¶€ì¡±í•  ê²½ìš°ì—ëŠ” 'ì œê³µëœ ì •ë³´ë¡œëŠ” ì •í™•íˆ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ë§Œ, ì¼ë°˜ì ìœ¼ë¡œ...'ë¼ëŠ” í˜•ì‹ìœ¼ë¡œ ë‹µí•˜ë¼.

ë‹¤ìŒì€ ì§ˆë¬¸ê³¼ ë‹µë³€ ì˜ˆì‹œì´ë‹¤:
ì˜ˆì‹œ 1:
ì§ˆë¬¸: íƒ€ì´ë ˆë†€ì˜ íš¨ëŠ¥ì€ ë¬´ì—‡ì¸ê°€ìš”?
ë‹µë³€: íƒ€ì´ë ˆë†€ì€ ë‘í†µ ë° ë°œì—´ì„ ì™„í™”í•˜ëŠ” ë° íš¨ê³¼ì ì´ì—ìš”. íŠ¹íˆ ê°ê¸° ì¦ìƒì„ ì™„í™”í•˜ëŠ” ë° ìì£¼ ì‚¬ìš©ë©ë‹ˆë‹¤.

ì˜ˆì‹œ 2:
ì§ˆë¬¸: ì´ë¶€í”„ë¡œíœì˜ ë³µìš©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?
ë‹µë³€: ì´ë¶€í”„ë¡œíœì€ ì„±ì¸ì˜ ê²½ìš° 1ì¼ 3íšŒ, ì‹ì‚¬ í›„ ë³µìš©í•˜ëŠ” ê²ƒì´ ê¶Œì¥ë¼ìš”. ì‹ì‚¬ì™€ í•¨ê»˜ ë³µìš©í•˜ë©´ ìœ„ì¥ ì¥ì• ë¥¼ ì¤„ì¼ ìˆ˜ ìˆì–´ìš”.

ì°¸ê³ í•  ìˆ˜ ìˆëŠ” ì •ë³´:
{context}
"""
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=question),
    ]

    # 4) ëª¨ë¸ ì§ˆì˜
    response = model.invoke(messages)
    return response.content

# ------------------------------------------------------------------
# [6] ë©”ì¸ ì‹¤í–‰ë¶€
# ------------------------------------------------------------------
if __name__ == "__main__":
    file_path = "./edruginfo.xlsx"

    # ë°ì´í„° ë¡œë“œ
    documents = load_and_prepare_data(file_path)

    # ì„ë² ë”© + ìºì‹±
    vectors, filtered_documents = build_vector_store(documents)

    # LLM & Embedding ëª¨ë¸ ì¤€ë¹„
    llm_model = ChatOpenAI(model='gpt-4o-mini', temperature=0)
    embedding_model = OpenAIEmbeddings(model='text-embedding-3-large')

    while True:
        question = input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥): ")
        if question.lower() == 'exit':
            break

        answer = query_rag(question, vectors, filtered_documents, llm_model, embedding_model, k=5)
        print("\nA:", answer)
