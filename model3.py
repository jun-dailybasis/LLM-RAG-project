import os
import pickle
import numpy as np
import pandas as pd
from langchain_openai import ChatOpenAI
from langchain.docstore.document import Document
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.schema import SystemMessage, HumanMessage
from sklearn.metrics.pairwise import cosine_similarity

# ------------------------------------------------------------------
# [1] ì„ë² ë”© ìºì‹œ ë¡œë“œ/ì €ì¥
# ------------------------------------------------------------------
EMBEDDING_CACHE_FILE = "./embeddings_cache2.pkl"

def load_embeddings_cache():
    """ ë¡œì»¬ ìºì‹œ íŒŒì¼ì—ì„œ ì„ë² ë”© ì •ë³´ë¥¼ ë¡œë“œ """
    if os.path.exists(EMBEDDING_CACHE_FILE):
        with open(EMBEDDING_CACHE_FILE, "rb") as f:
            return pickle.load(f)
    return {}

def save_embeddings_cache(cache):
    """ ë¡œì»¬ ìºì‹œì— ì„ë² ë”© ì •ë³´ë¥¼ ì €ì¥ """
    with open(EMBEDDING_CACHE_FILE, "wb") as f:
        pickle.dump(cache, f)

# ------------------------------------------------------------------
# [2] í…ìŠ¤íŠ¸ chunking í•¨ìˆ˜ (ê¸€ì ìˆ˜ ê¸°ì¤€)
# ------------------------------------------------------------------
def chunk_text_by_length(text, max_length=500):
    """
    í…ìŠ¤íŠ¸ë¥¼ ê¸€ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ chunkingí•©ë‹ˆë‹¤.
    max_lengthëŠ” ê° chunkì˜ ìµœëŒ€ ê¸€ì ìˆ˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
    """
    chunks = []
    for i in range(0, len(text), max_length):
        chunks.append(text[i:i+max_length].strip())
    return chunks

# ------------------------------------------------------------------
# [3] ì—‘ì…€ ë¡œë“œ â†’ (body ìƒì„±) â†’ chunking â†’ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
# ------------------------------------------------------------------
def load_and_prepare_data(file_path):
    df = pd.read_excel(file_path, engine="openpyxl")

    # 1) Body ì •ì˜
    df['body'] = df.apply(
        lambda row: (
            f"ì œí’ˆëª…: {row['ì œí’ˆëª…'] if pd.notnull(row['ì œí’ˆëª…']) else 'ì •ë³´ ì—†ìŒ'}\n"
            f"ì œì¡°ì—…ì²´: {row['ì—…ì²´ëª…'] if pd.notnull(row['ì—…ì²´ëª…']) else 'ì •ë³´ ì—†ìŒ'}\n"
            f"ì£¼ì„±ë¶„: {row['ì£¼ì„±ë¶„'] if pd.notnull(row['ì£¼ì„±ë¶„']) else 'ì •ë³´ ì—†ìŒ'}\n"
            f"íš¨ëŠ¥: {row['ì´ ì•½ì˜ íš¨ëŠ¥ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?'] if pd.notnull(row['ì´ ì•½ì˜ íš¨ëŠ¥ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?']) else 'ì •ë³´ ì—†ìŒ'}\n"
            f"ë³µìš©ë²•: {row['ì´ ì•½ì€ ì–´ë–»ê²Œ ì‚¬ìš©í•©ë‹ˆê¹Œ?'] if pd.notnull(row['ì´ ì•½ì€ ì–´ë–»ê²Œ ì‚¬ìš©í•©ë‹ˆê¹Œ?']) else 'ì •ë³´ ì—†ìŒ'}\n"
            f"ì‚¬ìš© ì „ ì£¼ì˜ì‚¬í•­: {row['ì´ ì•½ì„ ì‚¬ìš©í•˜ê¸° ì „ì— ë°˜ë“œì‹œ ì•Œì•„ì•¼ í•  ë‚´ìš©ì€ ë¬´ì—‡ì…ë‹ˆê°€?'] if pd.notnull(row['ì´ ì•½ì„ ì‚¬ìš©í•˜ê¸° ì „ì— ë°˜ë“œì‹œ ì•Œì•„ì•¼ í•  ë‚´ìš©ì€ ë¬´ì—‡ì…ë‹ˆê°€?']) else 'ì •ë³´ ì—†ìŒ'}\n"
            f"ì‚¬ìš©ìƒ ì£¼ì˜ì‚¬í•­: {row['ì´ ì•½ì˜ ì‚¬ìš©ìƒ ì£¼ì˜ì‚¬í•­ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?'] if pd.notnull(row['ì´ ì•½ì˜ ì‚¬ìš©ìƒ ì£¼ì˜ì‚¬í•­ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?']) else 'ì •ë³´ ì—†ìŒ'}\n"
            f"ì£¼ì˜í•´ì•¼ í•  ì•½/ìŒì‹: {row['ì´ ì•½ì„ ì‚¬ìš©í•˜ëŠ” ë™ì•ˆ ì£¼ì˜í•´ì•¼ í•  ì•½ ë˜ëŠ” ìŒì‹ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?'] if pd.notnull(row['ì´ ì•½ì„ ì‚¬ìš©í•˜ëŠ” ë™ì•ˆ ì£¼ì˜í•´ì•¼ í•  ì•½ ë˜ëŠ” ìŒì‹ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?']) else 'ì •ë³´ ì—†ìŒ'}\n"
            f"ë¶€ì‘ìš©: {row['ì´ ì•½ì€ ì–´ë–¤ ì´ìƒë°˜ì‘ì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆê¹Œ?'] if pd.notnull(row['ì´ ì•½ì€ ì–´ë–¤ ì´ìƒë°˜ì‘ì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆê¹Œ?']) else 'ì •ë³´ ì—†ìŒ'}\n"
            f"ë³´ê´€ë°©ë²•: {row['ì´ ì•½ì€ ì–´ë–»ê²Œ ë³´ê´€í•´ì•¼ í•©ë‹ˆê¹Œ?'] if pd.notnull(row['ì´ ì•½ì€ ì–´ë–»ê²Œ ë³´ê´€í•´ì•¼ í•©ë‹ˆê¹Œ?']) else 'ì •ë³´ ì—†ìŒ'}\n"
        ),
        axis=1
)

    # 2) chunking í›„ Document ê°ì²´ë¡œ ë³€í™˜
    documents = []
    for _, row in df.iterrows():
        body_text = row['body']
        chunks = chunk_text_by_length(body_text, max_length=500)
        for chunk in chunks:
            if chunk.strip():  # ë¹ˆ ë¬¸ì¥ì€ ë¬´ì‹œ
                documents.append(
                    Document(
                        page_content=chunk.strip(),
                        metadata={
                            "product_name": row['ì œí’ˆëª…'] if pd.notnull(row['ì œí’ˆëª…']) else 'ì •ë³´ ì—†ìŒ',
                            "company_name": row['ì—…ì²´ëª…'] if pd.notnull(row['ì—…ì²´ëª…']) else 'ì •ë³´ ì—†ìŒ',
                            "main_ingredient": row['ì£¼ì„±ë¶„'] if pd.notnull(row['ì£¼ì„±ë¶„']) else 'ì •ë³´ ì—†ìŒ'
                        }
                    )
                )
    return documents

# ------------------------------------------------------------------
# [4] ì„ë² ë”© & ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
# ------------------------------------------------------------------
def build_vector_store(documents):
    """
    ì „ì²´ documents ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•´ ì„ë² ë”©ì„ ìƒì„±/ë¡œë“œí•˜ê³ 
    numpy array(vectors)ì™€ ë¬¸ì„œ ëª©ë¡(filtered_documents)ì„ ë°˜í™˜.
    """
    embedding_model = OpenAIEmbeddings(model='text-embedding-ada-002')
    embedding_cache = load_embeddings_cache()
    new_embeddings = {}

    vectors = []
    filtered_documents = []

    for doc in documents:
        content = doc.page_content
        if content in embedding_cache:
            print(f"âœ… Using cached embedding for: {content[:30]}...")
            vectors.append(embedding_cache[content])
        else:
            print(f"ğŸš¨ Generating new embedding for: {content[:30]}...")
            embedding = embedding_model.embed_query(content)
            new_embeddings[content] = embedding
            vectors.append(embedding)

        filtered_documents.append(doc)

    # ìƒˆë¡œ ìƒì„±ëœ ì„ë² ë”©ì„ ìºì‹œì— ì—…ë°ì´íŠ¸í•˜ê³  ì €ì¥
    embedding_cache.update(new_embeddings)
    save_embeddings_cache(embedding_cache)

    return np.array(vectors), filtered_documents

# ------------------------------------------------------------------
# [5] ìœ ì‚¬ë„ ê²€ìƒ‰ í•¨ìˆ˜
# ------------------------------------------------------------------
def similarity_search(question, vectors, documents, embedding_model, top_k=5):
    """
    ì§ˆë¬¸ì„ ì„ë² ë”©í•´, ì „ì²´ ë²¡í„° ì¤‘ ìƒìœ„ top_kê°œë¥¼ ê³¨ë¼ ê·¸ ë¬¸ì„œë¥¼ ë°˜í™˜
    """
    question_embedding = embedding_model.embed_query(question)
    similarities = cosine_similarity([question_embedding], vectors)[0]
    top_indices = np.argsort(similarities)[::-1][:top_k]
    return [documents[i] for i in top_indices]

# ------------------------------------------------------------------
# [6] RAG ê¸°ë°˜ ì§ˆì˜ ì‘ë‹µ
# ------------------------------------------------------------------
def query_rag(question, vectors, documents, model, embedding_model, k=5):
    # 1) ìœ ì‚¬ë„ ê²€ìƒ‰
    search_results = similarity_search(question, vectors, documents, embedding_model, top_k=k)

    if not search_results:
        return "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # 2) ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš© í•©ì¹˜ê¸°
    context = "\n\n".join([doc.page_content for doc in search_results])

    # 3) ê°œì„ ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    system_prompt = f"""
ë„ˆëŠ” ì „ë¬¸ ì•½ì‚¬(Pharmacist) ì—­í• ì„ ë§¡ê³  ìˆë‹¤.
ì‚¬ìš©ìì˜ ì˜ì•½í’ˆ ê´€ë ¨ ì§ˆë¬¸ì— ëŒ€í•´, ì•„ë˜ ì œê³µëœ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬
ì „ë¬¸ì ì´ë©´ì„œë„ ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹µë³€í•´ë¼.

- ì œê³µëœ ì •ë³´ì— ì—†ëŠ” ë‚´ìš©ì´ë‚˜ í™•ì‹ ì´ ì—†ëŠ” ë¶€ë¶„ì— ëŒ€í•´ì„œëŠ” 'ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤' ë˜ëŠ” 'í•´ë‹¹ ì •ë³´ë¡œëŠ” ë‹µí•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤'ë¼ê³  ëª…í™•í•˜ê²Œ ë‹µí•˜ë¼.
- ë°˜ë“œì‹œ ì œê³µëœ ì •ë³´ë§Œì„ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ë¼.
- ë‹µë³€ì€ ê°„ê²°í•˜ë©´ì„œë„ êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•˜ë¼.

ì°¸ê³ í•  ìˆ˜ ìˆëŠ” ì •ë³´(Chunk)ë“¤:
{context}
"""

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=question),
    ]

    # 4) ëª¨ë¸ ì§ˆì˜
    response = model.invoke(messages)
    return response.content

# ------------------------------------------------------------------
# [7] ë©”ì¸ ì‹¤í–‰ë¶€
# ------------------------------------------------------------------
if __name__ == "__main__":
    file_path = "./edruginfo.xlsx"

    # ë°ì´í„° ë¡œë“œ + chunking
    documents = load_and_prepare_data(file_path)

    # ì„ë² ë”© + ìºì‹±
    vectors, filtered_documents = build_vector_store(documents)

    # LLM & Embedding ëª¨ë¸ ì¤€ë¹„
    llm_model = ChatOpenAI(model='gpt-4o-mini', temperature=0)
    embedding_model = OpenAIEmbeddings(model='text-embedding-ada-002')

    while True:
        question = input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥): ")
        if question.lower() == 'exit':
            break

        answer = query_rag(question, vectors, filtered_documents, llm_model, embedding_model, k=5)
        print("\nA:", answer)
