import os
import pickle
import numpy as np
from langchain_openai import ChatOpenAI
from langchain.docstore.document import Document
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.schema import SystemMessage, HumanMessage
from sklearn.metrics.pairwise import cosine_similarity

import pdb
import re
# ì„ë² ë”© ìºì‹œ íŒŒì¼ ê²½ë¡œ
EMBEDDING_CACHE_FILE = "./embeddings_cache.pkl"

# ìºì‹œ ë¡œë“œ/ì €ì¥ í•¨ìˆ˜
def load_embeddings_cache():
    if os.path.exists(EMBEDDING_CACHE_FILE):
        with open(EMBEDDING_CACHE_FILE, "rb") as f:
            return pickle.load(f)
    return {}

def save_embeddings_cache(cache):
    with open(EMBEDDING_CACHE_FILE, "wb") as f:
        pickle.dump(cache, f)

# ë°ì´í„° ë¡œë“œ ë° ë¬¸ì„œ ë³€í™˜
def load_and_prepare_data(file_path):
    import pandas as pd
    df = pd.read_excel(file_path, engine="openpyxl")
    df['body'] = (
        "ì œí’ˆëª…ì€ " + df['ì œí’ˆëª…'].fillna('') + "ì…ë‹ˆë‹¤. "
        "ì œì¡°ì—…ì²´ëŠ” " + df['ì—…ì²´ëª…'].fillna('') + "ì…ë‹ˆë‹¤. "
        "ì£¼ì„±ë¶„ì€ " + df['ì£¼ì„±ë¶„'].fillna('') + "ì…ë‹ˆë‹¤. "
        "ì´ ì•½ì˜ íš¨ëŠ¥ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. " + df['ì´ ì•½ì˜ íš¨ëŠ¥ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?'].fillna('') + ". "
        "ì´ ì•½ì˜ ë³µìš©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. " + df['ì´ ì•½ì€ ì–´ë–»ê²Œ ì‚¬ìš©í•©ë‹ˆê¹Œ?'].fillna('') + ". "
        "ì‚¬ìš© ì „ ì£¼ì˜ì‚¬í•­ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. " + df['ì´ ì•½ì„ ì‚¬ìš©í•˜ê¸° ì „ì— ë°˜ë“œì‹œ ì•Œì•„ì•¼ í•  ë‚´ìš©ì€ ë¬´ì—‡ì…ë‹ˆê°€?'].fillna('') + ". "
        "ì£¼ì˜ ì‚¬í•­ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. " + df['ì´ ì•½ì˜ ì‚¬ìš©ìƒ ì£¼ì˜ì‚¬í•­ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?'].fillna('') + ". "
        "ì£¼ì˜í•´ì•¼ í•  ì•½ì´ë‚˜ ìŒì‹ì€ " + df['ì´ ì•½ì„ ì‚¬ìš©í•˜ëŠ” ë™ì•ˆ ì£¼ì˜í•´ì•¼ í•  ì•½ ë˜ëŠ” ìŒì‹ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?'].fillna('') + "ì…ë‹ˆë‹¤. "
        "ì´ ì•½ì€ ë‹¤ìŒê³¼ ê°™ì€ ë¶€ì‘ìš©ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. " + df['ì´ ì•½ì€ ì–´ë–¤ ì´ìƒë°˜ì‘ì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆê¹Œ?'].fillna('') + ". "
        "ë³´ê´€ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. " + df['ì´ ì•½ì€ ì–´ë–»ê²Œ ë³´ê´€í•´ì•¼ í•©ë‹ˆê¹Œ?'].fillna('') + "."
    )
    documents = [
        Document(page_content=row['body'], metadata={"product_name": row['ì œí’ˆëª…'], "company_name": row['ì—…ì²´ëª…']})
        for _, row in df.iterrows()
    ]
    return documents

# ë²¡í„° ìŠ¤í† ì–´ ìƒì„± (ìºì‹œì™€ OpenAI API í˜¼ìš©)

def preprocess_text(text):
    # ì¤„ ë°”ê¿ˆê³¼ íƒ­ ì œê±° (ì—”í„° ì œê±° í¬í•¨)
    text = text.replace("\n", " ").replace("\t", " ")
    
    # ì—°ì†ëœ ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text)

    # ë¶ˆí•„ìš”í•œ ë§ˆì¹¨í‘œ ì œê±°
    # ì—°ì†ëœ ë§ˆì¹¨í‘œ ì²˜ë¦¬: `...` -> `.`
    text = re.sub(r'([.!?])\1+', r'\1', text)
    # ë§ˆì¹¨í‘œê°€ ë‹¨ë…ìœ¼ë¡œ ì¡´ì¬í•˜ëŠ” ê²½ìš° ì œê±°
    text = re.sub(r'\s+\. ', ' ', text)
    text = re.sub(r'\.\s+\.', '.', text)

    # ë¬¸ì¥ ê¸°í˜¸ì™€ ê³µë°± ì •ë¦¬
    # ë¬¸ì¥ ê¸°í˜¸ ì•ì˜ ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    text = re.sub(r'\s+([.,!?])', r'\1', text)
    # ë¬¸ì¥ ê¸°í˜¸ ë’¤ì˜ ë¶ˆí•„ìš”í•œ ê³µë°± ì¡°ì • (ë‹¨ì¼ ê³µë°± ìœ ì§€)
    text = re.sub(r'([.,!?])\s+', r'\1 ', text)

    # ë¬¸ìì—´ ì–‘ ëì˜ ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    text = text.strip()

    return text


# ë²¡í„° ìŠ¤í† ì–´ ìƒì„± (ìºì‹œì™€ OpenAI API í˜¼ìš©)
def build_vector_store(documents):
    embedding_model = OpenAIEmbeddings(model='text-embedding-3-large')
    embedding_cache = load_embeddings_cache()
    new_embeddings = {}

    vectors = []
    filtered_documents = []

    for doc in documents:
        content = doc.page_content
        content = preprocess_text(content)
        # pdb.set_trace()

        if content in embedding_cache:
            print(f"âœ… Using cached embedding for: {content[:30]}...")
            vectors.append(embedding_cache[content])
        else:
            print(f"ğŸš¨ Generating new embedding for: {content[:30]}...")
            embedding = embedding_model.embed_query(content)
            new_embeddings[content] = embedding
            vectors.append(embedding)

        filtered_documents.append(doc)

    # ìºì‹œ ì—…ë°ì´íŠ¸ ë° ì €ì¥
    embedding_cache.update(new_embeddings)
    save_embeddings_cache(embedding_cache)

    return np.array(vectors), filtered_documents

# ìœ ì‚¬ë„ ê²€ìƒ‰ í•¨ìˆ˜
def similarity_search(question, vectors, documents, embedding_model, top_k=5):
    question_embedding = embedding_model.embed_query(question)
    similarities = cosine_similarity([question_embedding], vectors)[0]
    top_indices = np.argsort(similarities)[::-1][:top_k]

    return [documents[i] for i in top_indices]

# í‚¤ì›Œë“œ ê²€ìƒ‰ í•¨ìˆ˜
def keyword_search(question, documents):
    keywords = question.split()  # ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
    return [
        doc for doc in documents
        if any(keyword in doc.page_content for keyword in keywords)
    ]

# RAG ì¿¼ë¦¬ í•¨ìˆ˜
def query_rag(question, vectors, documents, model, embedding_model, k=5):
    # Step 1: ìœ ì‚¬ë„ ê²€ìƒ‰
    search_results = similarity_search(question, vectors, documents, embedding_model, top_k=k)
    
    # Step 2: í‚¤ì›Œë“œ ê²€ìƒ‰
    keyword_results = keyword_search(question, documents)
    
    # Step 3: ê²€ìƒ‰ ê²°ê³¼ ë³‘í•© (ì¤‘ë³µ ì œê±°)
    combined_results = {doc.page_content: doc for doc in search_results + keyword_results}.values()
    
    if not combined_results:
        return "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš© í•©ì¹˜ê¸°
    context = "\n\n".join([result.page_content for result in combined_results])
    system_prompt = (
        "ë‹¤ìŒ ì •ë³´ë¥¼ ì°¸ê³ í•´ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”. "
        "ë§Œì•½ í•´ë‹¹ ë‚´ìš©ì´ ë¶€ì¡±í•˜ê±°ë‚˜ í™•ì‹ ì´ ì—†ë‹¤ë©´, ëª¨ë¥¸ë‹¤ê³  ë‹µí•´ì£¼ì„¸ìš”.\n\n"
        f"{context}"
    )

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=question),
    ]
    response = model.invoke(messages)
    return response.content

# ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    file_path = "./edruginfo.xlsx"
    documents = load_and_prepare_data(file_path)
    vectors, filtered_documents = build_vector_store(documents)
    llm_model = ChatOpenAI(model='gpt-4o-mini', temperature=0)
    embedding_model = OpenAIEmbeddings(model='text-embedding-3-large')

    while True:
        question = input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥): ")
        if question.lower() == 'exit':
            break
        answer = query_rag(question, vectors, filtered_documents, llm_model, embedding_model, k=5)
        print("\nA:", answer)
